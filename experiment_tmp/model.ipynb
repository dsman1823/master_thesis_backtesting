{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlApRJuPfrxq"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EvaaDKBoMaO8",
    "outputId": "11c8de58-a8b7-4df1-ffd1-22e6b823df02"
   },
   "outputs": [],
   "source": [
    "!pip uninstall deepdowmine -y\n",
    "!pip install git+https://github.com/dsman1823/deepdowmine.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tfN21pcMhatf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from deepdowmine.data import InRAMDataset, RigidDataLoader, prepare_standard_scaler, Scale, SeqRigidDataLoader, WeeklyRigidDataLoader\n",
    "from deepdowmine.losses import MeanReturns, SharpeRatio, MaximumDrawdown, StandardDeviation\n",
    "from deepdowmine.nn import DenseNetFullOpti2, DenseNetMinVar2, RnnNetMinVar2, RnnNetMinVar, DenseNetFullOpti2, RnnNetFullOpti2, ConvNetFullOpti2\n",
    "from deepdowmine.nn import LstmNetMinVar2, LstmNetFullOpti2, RnnNetMinVar3, RnnNetFullOpti3\n",
    "from deepdowmine.experiments import Run\n",
    "from deepdowmine.callbacks import EarlyStoppingCallback\n",
    "from deepdowmine.visualize import generate_metrics_table, generate_weights_table, plot_metrics, plot_weight_heatmap\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNqxm-tdcv3Y"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "x-bSnRLxonV_"
   },
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "returns = pd.read_csv('train_data_2.csv', index_col = 0).to_numpy()[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xr7u61ueYxcB",
    "outputId": "e1e9183a-1971-46b5-ab77-f79f8ba6bb65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4015"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "5G-3j5QtgEH6"
   },
   "outputs": [],
   "source": [
    "n_timesteps, n_assets = len(returns), 5#11**4, 450\n",
    "\n",
    "#returns = np.random.normal(0, .2, size = (n_timesteps, n_assets))\n",
    "\n",
    "lookback, gap, horizon = 50, 0, 5# 40, 0, 5   loss=-0.09645, test_loss=-0.08003]\n",
    "n_samples = n_timesteps - lookback - horizon - gap + 1\n",
    "\n",
    "indices = np.arange(n_samples)\n",
    "split_ix = int(n_samples * 0.8)\n",
    "indices_train = indices[:split_ix]\n",
    "indices_test = indices[split_ix:]\n",
    "\n",
    "\n",
    "# print('Train range: {}:{}\\nTest range: {}:{}'.format(indices_train[0], indices_train[-1],\n",
    "#                                                      indices_test[0], indices_test[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "3WWSpDn2IuhS"
   },
   "outputs": [],
   "source": [
    "def transform_returns_to_Xy_tensors(returns, lookback, n_timesteps, horizon, gap):\n",
    "    X_list, y_list = [], []\n",
    "\n",
    "    for i in range(lookback, n_timesteps - horizon - gap + 1):\n",
    "        X_list.append(returns[i - lookback: i, :])\n",
    "        y_list.append(returns[i + gap: i + gap + horizon, :])\n",
    "\n",
    "    X = np.stack(X_list, axis=0)[:, None, ...]\n",
    "    y = np.stack(y_list, axis=0)[:, None, ...]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "lw1e3TQXgHI5"
   },
   "outputs": [],
   "source": [
    "\n",
    "#returns = np.random.normal(0, .2, size = (n_timesteps, n_assets))\n",
    "\n",
    "# X_list, y_list = [], []\n",
    "\n",
    "# for i in range(lookback, n_timesteps - horizon - gap + 1):\n",
    "#     X_list.append(returns[i - lookback: i, :])\n",
    "#     y_list.append(returns[i + gap: i + gap + horizon, :])\n",
    "\n",
    "# X = np.stack(X_list, axis=0)[:, None, ...]\n",
    "# y = np.stack(y_list, axis=0)[:, None, ...]\n",
    "X, y = transform_returns_to_Xy_tensors(returns, lookback, n_timesteps, horizon, gap)\n",
    "#print('X: {}, y: {}'.format(X.shape, y.shape))\n",
    "\n",
    "# means, stds = prepare_standard_scaler(X, indices=indices_train)\n",
    "# print('mean: {}, std: {}'.format(means, stds))\n",
    "\n",
    "dataset = InRAMDataset(X, y)\n",
    "#, transform=Scale(means, stds))\n",
    "\n",
    "torch.manual_seed(32)\n",
    "\n",
    "dataloader_train = RigidDataLoader(dataset,\n",
    "                                   indices=indices_train,\n",
    "                                   batch_size=32)\n",
    "\n",
    "dataloader_test = SeqRigidDataLoader(dataset,\n",
    "                                  indices=indices_test,\n",
    "                                  batch_size=32)\n",
    "\n",
    "dataloader_train_for_retrain = RigidDataLoader(dataset,\n",
    "                                  indices=indices_test,\n",
    "                                  batch_size=32)\n",
    "dataloader_test_for_retrain = SeqRigidDataLoader(dataset,\n",
    "                                   indices=indices_train,\n",
    "                                   batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ka29tJgfco0Z"
   },
   "source": [
    "# Network setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOuNfpMXgcKN",
    "outputId": "c116905c-4f66-43bc-e8e2-fd7c37945835"
   },
   "outputs": [],
   "source": [
    "network = RnnNetFullOpti3(5,  0.2, 'diagonal', 1)\n",
    "print(network)\n",
    "network = network.train()\n",
    "loss = SharpeRatio()\n",
    "run = Run(network,\n",
    "          loss,\n",
    "          dataloader_train,\n",
    "          val_dataloaders={\n",
    "              'test': dataloader_test,\n",
    "              'train': dataloader_train\n",
    "              },\n",
    "          optimizer=torch.optim.Adam(network.parameters(), amsgrad=True, lr=0.001),\n",
    "          callbacks=[EarlyStoppingCallback(metric_name='loss',\n",
    "                                           dataloader_name='test',\n",
    "                                           patience=7)]) #15 # patience controlls amount offffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffrfvd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sL_SktboD7l8"
   },
   "source": [
    "# Train start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wo4B68DGhMjY",
    "outputId": "83c2c8a2-3768-4416-8c7b-d1c62725dc21"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:  12%|███████▏                                                   | 12/99 [00:09<01:11,  1.22it/s, loss=0.00084]\u001b[A\n",
      "\n",
      "Epoch 1:   1%|▋                                                                         | 1/99 [00:00<00:46,  2.09it/s]\u001b[A\n",
      "Epoch 1:   1%|▌                                                          | 1/99 [00:00<00:46,  2.09it/s, loss=-0.04344]\u001b[A\n",
      "Epoch 1:   2%|█▏                                                         | 2/99 [00:00<00:46,  2.08it/s, loss=-0.04344]\u001b[A\n",
      "Epoch 1:   2%|█▏                                                         | 2/99 [00:00<00:46,  2.08it/s, loss=-0.03506]\u001b[A\n",
      "Epoch 1:   3%|█▊                                                         | 3/99 [00:01<00:46,  2.06it/s, loss=-0.03506]\u001b[A\n",
      "Epoch 1:   3%|█▊                                                         | 3/99 [00:01<00:46,  2.06it/s, loss=-0.04864]\u001b[A\n",
      "Epoch 1:   4%|██▍                                                        | 4/99 [00:01<00:45,  2.08it/s, loss=-0.04864]\u001b[A\n",
      "Epoch 1:   4%|██▍                                                        | 4/99 [00:01<00:45,  2.08it/s, loss=-0.08069]\u001b[A\n",
      "Epoch 1:   5%|██▉                                                        | 5/99 [00:02<00:44,  2.09it/s, loss=-0.08069]\u001b[A\n",
      "Epoch 1:   5%|██▉                                                        | 5/99 [00:02<00:44,  2.09it/s, loss=-0.06901]\u001b[A\n",
      "Epoch 1:   6%|███▌                                                       | 6/99 [00:02<00:44,  2.11it/s, loss=-0.06901]\u001b[A\n",
      "Epoch 1:   6%|███▌                                                       | 6/99 [00:02<00:44,  2.11it/s, loss=-0.07297]\u001b[A\n",
      "Epoch 1:   7%|████▏                                                      | 7/99 [00:03<00:43,  2.12it/s, loss=-0.07297]\u001b[A\n",
      "Epoch 1:   7%|████▏                                                      | 7/99 [00:03<00:43,  2.12it/s, loss=-0.06964]\u001b[A\n",
      "Epoch 1:   8%|████▊                                                      | 8/99 [00:03<00:43,  2.11it/s, loss=-0.06964]\u001b[A\n",
      "Epoch 1:   8%|████▊                                                      | 8/99 [00:03<00:43,  2.11it/s, loss=-0.07024]\u001b[A\n",
      "Epoch 1:   9%|█████▎                                                     | 9/99 [00:04<00:42,  2.14it/s, loss=-0.07024]\u001b[A\n",
      "Epoch 1:   9%|█████▎                                                     | 9/99 [00:04<00:42,  2.14it/s, loss=-0.05063]\u001b[A\n",
      "Epoch 1:  10%|█████▊                                                    | 10/99 [00:04<00:41,  2.13it/s, loss=-0.05063]\u001b[A\n",
      "Epoch 1:  10%|█████▊                                                    | 10/99 [00:04<00:41,  2.13it/s, loss=-0.04228]\u001b[A\n",
      "Epoch 1:  11%|██████▍                                                   | 11/99 [00:05<00:41,  2.13it/s, loss=-0.04228]\u001b[A\n",
      "Epoch 1:  11%|██████▍                                                   | 11/99 [00:05<00:41,  2.13it/s, loss=-0.03735]\u001b[A\n",
      "Epoch 1:  12%|███████                                                   | 12/99 [00:05<00:40,  2.14it/s, loss=-0.03735]\u001b[A\n",
      "Epoch 1:  12%|███████                                                   | 12/99 [00:05<00:40,  2.14it/s, loss=-0.02648]\u001b[A\n",
      "Epoch 1:  13%|███████▌                                                  | 13/99 [00:06<00:40,  2.14it/s, loss=-0.02648]\u001b[A\n",
      "Epoch 1:  13%|███████▌                                                  | 13/99 [00:06<00:40,  2.14it/s, loss=-0.01792]\u001b[A\n",
      "Epoch 1:  14%|████████▏                                                 | 14/99 [00:06<00:39,  2.15it/s, loss=-0.01792]\u001b[A\n",
      "Epoch 1:  14%|████████▏                                                 | 14/99 [00:06<00:39,  2.15it/s, loss=-0.00835]\u001b[A\n",
      "Epoch 1:  15%|████████▊                                                 | 15/99 [00:07<00:39,  2.15it/s, loss=-0.00835]\u001b[A\n",
      "Epoch 1:  15%|████████▊                                                 | 15/99 [00:07<00:39,  2.15it/s, loss=-0.01757]\u001b[A\n",
      "Epoch 1:  16%|█████████▎                                                | 16/99 [00:07<00:38,  2.15it/s, loss=-0.01757]\u001b[A\n",
      "Epoch 1:  16%|█████████▎                                                | 16/99 [00:07<00:38,  2.15it/s, loss=-0.02876]\u001b[A\n",
      "Epoch 1:  17%|█████████▉                                                | 17/99 [00:08<00:39,  2.06it/s, loss=-0.02876]\u001b[A\n",
      "Epoch 1:  17%|█████████▉                                                | 17/99 [00:08<00:39,  2.06it/s, loss=-0.02777]\u001b[A\n",
      "Epoch 1:  18%|██████████▌                                               | 18/99 [00:08<00:38,  2.09it/s, loss=-0.02777]\u001b[A\n",
      "Epoch 1:  18%|██████████▌                                               | 18/99 [00:08<00:38,  2.09it/s, loss=-0.04754]\u001b[A\n",
      "Epoch 1:  19%|███████████▏                                              | 19/99 [00:09<00:38,  2.09it/s, loss=-0.04754]\u001b[A\n",
      "Epoch 1:  19%|███████████▏                                              | 19/99 [00:09<00:38,  2.09it/s, loss=-0.05058]\u001b[A\n",
      "Epoch 1:  20%|███████████▋                                              | 20/99 [00:09<00:37,  2.10it/s, loss=-0.05058]\u001b[A\n",
      "Epoch 1:  20%|███████████▋                                              | 20/99 [00:09<00:37,  2.10it/s, loss=-0.05304]\u001b[A\n",
      "Epoch 1:  21%|████████████▎                                             | 21/99 [00:09<00:37,  2.09it/s, loss=-0.05304]\u001b[A\n",
      "Epoch 1:  21%|████████████▎                                             | 21/99 [00:09<00:37,  2.09it/s, loss=-0.05667]\u001b[A\n",
      "Epoch 1:  22%|████████████▉                                             | 22/99 [00:10<00:36,  2.09it/s, loss=-0.05667]\u001b[A\n",
      "Epoch 1:  22%|████████████▉                                             | 22/99 [00:10<00:36,  2.09it/s, loss=-0.05598]\u001b[A\n",
      "Epoch 1:  23%|█████████████▍                                            | 23/99 [00:10<00:36,  2.10it/s, loss=-0.05598]\u001b[A\n",
      "Epoch 1:  23%|█████████████▍                                            | 23/99 [00:10<00:36,  2.10it/s, loss=-0.05507]\u001b[A\n",
      "Epoch 1:  24%|██████████████                                            | 24/99 [00:11<00:35,  2.11it/s, loss=-0.05507]\u001b[A\n",
      "Epoch 1:  24%|██████████████                                            | 24/99 [00:11<00:35,  2.11it/s, loss=-0.05118]\u001b[A\n",
      "Epoch 1:  25%|██████████████▋                                           | 25/99 [00:11<00:35,  2.10it/s, loss=-0.05118]\u001b[A\n",
      "Epoch 1:  25%|██████████████▋                                           | 25/99 [00:11<00:35,  2.10it/s, loss=-0.04920]\u001b[A\n",
      "Epoch 1:  26%|███████████████▏                                          | 26/99 [00:12<00:34,  2.14it/s, loss=-0.04920]\u001b[A\n",
      "Epoch 1:  26%|███████████████▏                                          | 26/99 [00:12<00:34,  2.14it/s, loss=-0.04750]\u001b[A\n",
      "Epoch 1:  27%|███████████████▊                                          | 27/99 [00:12<00:33,  2.15it/s, loss=-0.04750]\u001b[A\n",
      "Epoch 1:  27%|███████████████▊                                          | 27/99 [00:12<00:33,  2.15it/s, loss=-0.04631]\u001b[A\n",
      "Epoch 1:  28%|████████████████▍                                         | 28/99 [00:13<00:33,  2.14it/s, loss=-0.04631]\u001b[A\n",
      "Epoch 1:  28%|████████████████▍                                         | 28/99 [00:13<00:33,  2.14it/s, loss=-0.04584]\u001b[A\n",
      "Epoch 1:  29%|████████████████▉                                         | 29/99 [00:13<00:33,  2.12it/s, loss=-0.04584]\u001b[A\n",
      "Epoch 1:  29%|████████████████▉                                         | 29/99 [00:13<00:33,  2.12it/s, loss=-0.04961]\u001b[A\n",
      "Epoch 1:  30%|█████████████████▌                                        | 30/99 [00:14<00:32,  2.13it/s, loss=-0.04961]\u001b[A\n",
      "Epoch 1:  30%|█████████████████▌                                        | 30/99 [00:14<00:32,  2.13it/s, loss=-0.04752]\u001b[A\n",
      "Epoch 1:  31%|██████████████████▏                                       | 31/99 [00:14<00:31,  2.15it/s, loss=-0.04752]\u001b[A\n",
      "Epoch 1:  31%|██████████████████▏                                       | 31/99 [00:14<00:31,  2.15it/s, loss=-0.04487]\u001b[A\n",
      "Epoch 1:  32%|██████████████████▋                                       | 32/99 [00:15<00:30,  2.17it/s, loss=-0.04487]\u001b[A\n",
      "Epoch 1:  32%|██████████████████▋                                       | 32/99 [00:15<00:30,  2.17it/s, loss=-0.04381]\u001b[A\n",
      "Epoch 1:  33%|███████████████████▎                                      | 33/99 [00:15<00:31,  2.10it/s, loss=-0.04381]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  33%|███████████████████▎                                      | 33/99 [00:15<00:31,  2.10it/s, loss=-0.04702]\u001b[A\n",
      "Epoch 1:  34%|███████████████████▉                                      | 34/99 [00:16<00:31,  2.09it/s, loss=-0.04702]\u001b[A\n",
      "Epoch 1:  34%|███████████████████▉                                      | 34/99 [00:16<00:31,  2.09it/s, loss=-0.04913]\u001b[A\n",
      "Epoch 1:  35%|████████████████████▌                                     | 35/99 [00:16<00:30,  2.09it/s, loss=-0.04913]\u001b[A\n",
      "Epoch 1:  35%|████████████████████▌                                     | 35/99 [00:16<00:30,  2.09it/s, loss=-0.04772]\u001b[A\n",
      "Epoch 1:  36%|█████████████████████                                     | 36/99 [00:17<00:29,  2.13it/s, loss=-0.04772]\u001b[A\n",
      "Epoch 1:  36%|█████████████████████                                     | 36/99 [00:17<00:29,  2.13it/s, loss=-0.05085]\u001b[A\n",
      "Epoch 1:  37%|█████████████████████▋                                    | 37/99 [00:17<00:29,  2.13it/s, loss=-0.05085]\u001b[A\n",
      "Epoch 1:  37%|█████████████████████▋                                    | 37/99 [00:17<00:29,  2.13it/s, loss=-0.05050]\u001b[A\n",
      "Epoch 1:  38%|██████████████████████▎                                   | 38/99 [00:17<00:28,  2.15it/s, loss=-0.05050]\u001b[A\n",
      "Epoch 1:  38%|██████████████████████▎                                   | 38/99 [00:17<00:28,  2.15it/s, loss=-0.05210]\u001b[A\n",
      "Epoch 1:  39%|██████████████████████▊                                   | 39/99 [00:18<00:27,  2.16it/s, loss=-0.05210]\u001b[A\n",
      "Epoch 1:  39%|██████████████████████▊                                   | 39/99 [00:18<00:27,  2.16it/s, loss=-0.04708]\u001b[A\n",
      "Epoch 1:  40%|███████████████████████▍                                  | 40/99 [00:18<00:27,  2.16it/s, loss=-0.04708]\u001b[A\n",
      "Epoch 1:  40%|███████████████████████▍                                  | 40/99 [00:18<00:27,  2.16it/s, loss=-0.04377]\u001b[A\n",
      "Epoch 1:  41%|████████████████████████                                  | 41/99 [00:19<00:26,  2.18it/s, loss=-0.04377]\u001b[A\n",
      "Epoch 1:  41%|████████████████████████                                  | 41/99 [00:19<00:26,  2.18it/s, loss=-0.04105]\u001b[A\n",
      "Epoch 1:  42%|████████████████████████▌                                 | 42/99 [00:19<00:26,  2.18it/s, loss=-0.04105]\u001b[A\n",
      "Epoch 1:  42%|████████████████████████▌                                 | 42/99 [00:19<00:26,  2.18it/s, loss=-0.04015]\u001b[A\n",
      "Epoch 1:  43%|█████████████████████████▏                                | 43/99 [00:20<00:25,  2.17it/s, loss=-0.04015]\u001b[A\n",
      "Epoch 1:  43%|█████████████████████████▏                                | 43/99 [00:20<00:25,  2.17it/s, loss=-0.03987]\u001b[A\n",
      "Epoch 1:  44%|█████████████████████████▊                                | 44/99 [00:20<00:25,  2.17it/s, loss=-0.03987]\u001b[A\n",
      "Epoch 1:  44%|█████████████████████████▊                                | 44/99 [00:20<00:25,  2.17it/s, loss=-0.04035]\u001b[A\n",
      "Epoch 1:  45%|██████████████████████████▎                               | 45/99 [00:21<00:24,  2.19it/s, loss=-0.04035]\u001b[A\n",
      "Epoch 1:  45%|██████████████████████████▎                               | 45/99 [00:21<00:24,  2.19it/s, loss=-0.04437]\u001b[A\n",
      "Epoch 1:  46%|██████████████████████████▉                               | 46/99 [00:21<00:24,  2.19it/s, loss=-0.04437]\u001b[A\n",
      "Epoch 1:  46%|██████████████████████████▉                               | 46/99 [00:21<00:24,  2.19it/s, loss=-0.04614]\u001b[A\n",
      "Epoch 1:  47%|███████████████████████████▌                              | 47/99 [00:22<00:23,  2.19it/s, loss=-0.04614]\u001b[A\n",
      "Epoch 1:  47%|███████████████████████████▌                              | 47/99 [00:22<00:23,  2.19it/s, loss=-0.04513]\u001b[A\n",
      "Epoch 1:  48%|████████████████████████████                              | 48/99 [00:22<00:23,  2.19it/s, loss=-0.04513]\u001b[A\n",
      "Epoch 1:  48%|████████████████████████████                              | 48/99 [00:22<00:23,  2.19it/s, loss=-0.04632]\u001b[A\n",
      "Epoch 1:  49%|████████████████████████████▋                             | 49/99 [00:22<00:22,  2.20it/s, loss=-0.04632]\u001b[A\n",
      "Epoch 1:  49%|████████████████████████████▋                             | 49/99 [00:22<00:22,  2.20it/s, loss=-0.04818]\u001b[A\n",
      "Epoch 1:  51%|█████████████████████████████▎                            | 50/99 [00:23<00:22,  2.19it/s, loss=-0.04818]\u001b[A\n",
      "Epoch 1:  51%|█████████████████████████████▎                            | 50/99 [00:23<00:22,  2.19it/s, loss=-0.04877]\u001b[A\n",
      "Epoch 1:  52%|█████████████████████████████▉                            | 51/99 [00:23<00:22,  2.18it/s, loss=-0.04877]\u001b[A\n",
      "Epoch 1:  52%|█████████████████████████████▉                            | 51/99 [00:23<00:22,  2.18it/s, loss=-0.04984]\u001b[A\n",
      "Epoch 1:  53%|██████████████████████████████▍                           | 52/99 [00:24<00:21,  2.16it/s, loss=-0.04984]\u001b[A\n",
      "Epoch 1:  53%|██████████████████████████████▍                           | 52/99 [00:24<00:21,  2.16it/s, loss=-0.05263]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████████                           | 53/99 [00:24<00:21,  2.16it/s, loss=-0.05263]\u001b[A\n",
      "Epoch 1:  54%|███████████████████████████████                           | 53/99 [00:24<00:21,  2.16it/s, loss=-0.05269]\u001b[A\n",
      "Epoch 1:  55%|███████████████████████████████▋                          | 54/99 [00:25<00:20,  2.15it/s, loss=-0.05269]\u001b[A\n",
      "Epoch 1:  55%|███████████████████████████████▋                          | 54/99 [00:25<00:20,  2.15it/s, loss=-0.05341]\u001b[A\n",
      "Epoch 1:  56%|████████████████████████████████▏                         | 55/99 [00:25<00:20,  2.16it/s, loss=-0.05341]\u001b[A\n",
      "Epoch 1:  56%|████████████████████████████████▏                         | 55/99 [00:25<00:20,  2.16it/s, loss=-0.05184]\u001b[A\n",
      "Epoch 1:  57%|████████████████████████████████▊                         | 56/99 [00:26<00:20,  2.11it/s, loss=-0.05184]\u001b[A\n",
      "Epoch 1:  57%|████████████████████████████████▊                         | 56/99 [00:26<00:20,  2.11it/s, loss=-0.05713]\u001b[A\n",
      "Epoch 1:  58%|█████████████████████████████████▍                        | 57/99 [00:26<00:22,  1.88it/s, loss=-0.05713]\u001b[A\n",
      "Epoch 1:  58%|█████████████████████████████████▍                        | 57/99 [00:26<00:22,  1.88it/s, loss=-0.05694]\u001b[A\n",
      "Epoch 1:  59%|█████████████████████████████████▉                        | 58/99 [00:27<00:22,  1.82it/s, loss=-0.05694]\u001b[A\n",
      "Epoch 1:  59%|█████████████████████████████████▉                        | 58/99 [00:27<00:22,  1.82it/s, loss=-0.05513]\u001b[A\n",
      "Epoch 1:  60%|██████████████████████████████████▌                       | 59/99 [00:28<00:21,  1.88it/s, loss=-0.05513]\u001b[A\n",
      "Epoch 1:  60%|██████████████████████████████████▌                       | 59/99 [00:28<00:21,  1.88it/s, loss=-0.05632]\u001b[A\n",
      "Epoch 1:  61%|███████████████████████████████████▏                      | 60/99 [00:28<00:19,  1.96it/s, loss=-0.05632]\u001b[A\n",
      "Epoch 1:  61%|███████████████████████████████████▏                      | 60/99 [00:28<00:19,  1.96it/s, loss=-0.05416]\u001b[A\n",
      "Epoch 1:  62%|███████████████████████████████████▋                      | 61/99 [00:28<00:18,  2.01it/s, loss=-0.05416]\u001b[A\n",
      "Epoch 1:  62%|███████████████████████████████████▋                      | 61/99 [00:28<00:18,  2.01it/s, loss=-0.05542]\u001b[A\n",
      "Epoch 1:  63%|████████████████████████████████████▎                     | 62/99 [00:29<00:18,  2.03it/s, loss=-0.05542]\u001b[A\n",
      "Epoch 1:  63%|████████████████████████████████████▎                     | 62/99 [00:29<00:18,  2.03it/s, loss=-0.05734]\u001b[A\n",
      "Epoch 1:  64%|████████████████████████████████████▉                     | 63/99 [00:29<00:17,  2.06it/s, loss=-0.05734]\u001b[A\n",
      "Epoch 1:  64%|████████████████████████████████████▉                     | 63/99 [00:29<00:17,  2.06it/s, loss=-0.05596]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████████▍                    | 64/99 [00:30<00:16,  2.07it/s, loss=-0.05596]\u001b[A\n",
      "Epoch 1:  65%|█████████████████████████████████████▍                    | 64/99 [00:30<00:16,  2.07it/s, loss=-0.05788]\u001b[A\n",
      "Epoch 1:  66%|██████████████████████████████████████                    | 65/99 [00:30<00:16,  2.06it/s, loss=-0.05788]\u001b[A\n",
      "Epoch 1:  66%|██████████████████████████████████████                    | 65/99 [00:30<00:16,  2.06it/s, loss=-0.05622]\u001b[A\n",
      "Epoch 1:  67%|██████████████████████████████████████▋                   | 66/99 [00:31<00:15,  2.07it/s, loss=-0.05622]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  67%|██████████████████████████████████████▋                   | 66/99 [00:31<00:15,  2.07it/s, loss=-0.05635]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████████▎                  | 67/99 [00:31<00:15,  2.09it/s, loss=-0.05635]\u001b[A\n",
      "Epoch 1:  68%|███████████████████████████████████████▎                  | 67/99 [00:31<00:15,  2.09it/s, loss=-0.05272]\u001b[A\n",
      "Epoch 1:  69%|███████████████████████████████████████▊                  | 68/99 [00:32<00:14,  2.12it/s, loss=-0.05272]\u001b[A\n",
      "Epoch 1:  69%|███████████████████████████████████████▊                  | 68/99 [00:32<00:14,  2.12it/s, loss=-0.05573]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████████▍                 | 69/99 [00:32<00:14,  2.10it/s, loss=-0.05573]\u001b[A\n",
      "Epoch 1:  70%|████████████████████████████████████████▍                 | 69/99 [00:32<00:14,  2.10it/s, loss=-0.05557]\u001b[A\n",
      "Epoch 1:  71%|█████████████████████████████████████████                 | 70/99 [00:33<00:13,  2.14it/s, loss=-0.05557]\u001b[A\n",
      "Epoch 1:  71%|█████████████████████████████████████████                 | 70/99 [00:33<00:13,  2.14it/s, loss=-0.05579]\u001b[A\n",
      "Epoch 1:  72%|█████████████████████████████████████████▌                | 71/99 [00:33<00:12,  2.18it/s, loss=-0.05579]\u001b[A\n",
      "Epoch 1:  72%|█████████████████████████████████████████▌                | 71/99 [00:33<00:12,  2.18it/s, loss=-0.05845]\u001b[A\n",
      "Epoch 1:  73%|██████████████████████████████████████████▏               | 72/99 [00:34<00:12,  2.16it/s, loss=-0.05845]\u001b[A\n",
      "Epoch 1:  73%|██████████████████████████████████████████▏               | 72/99 [00:34<00:12,  2.16it/s, loss=-0.05708]\u001b[A\n",
      "Epoch 1:  74%|██████████████████████████████████████████▊               | 73/99 [00:34<00:12,  2.13it/s, loss=-0.05708]\u001b[A\n",
      "Epoch 1:  74%|██████████████████████████████████████████▊               | 73/99 [00:34<00:12,  2.13it/s, loss=-0.05609]\u001b[A\n",
      "Epoch 1:  75%|███████████████████████████████████████████▎              | 74/99 [00:35<00:11,  2.14it/s, loss=-0.05609]\u001b[A\n",
      "Epoch 1:  75%|███████████████████████████████████████████▎              | 74/99 [00:35<00:11,  2.14it/s, loss=-0.05686]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████████▉              | 75/99 [00:35<00:10,  2.19it/s, loss=-0.05686]\u001b[A\n",
      "Epoch 1:  76%|███████████████████████████████████████████▉              | 75/99 [00:35<00:10,  2.19it/s, loss=-0.05568]\u001b[A\n",
      "Epoch 1:  77%|████████████████████████████████████████████▌             | 76/99 [00:35<00:10,  2.19it/s, loss=-0.05568]\u001b[A\n",
      "Epoch 1:  77%|████████████████████████████████████████████▌             | 76/99 [00:35<00:10,  2.19it/s, loss=-0.05560]\u001b[A\n",
      "Epoch 1:  78%|█████████████████████████████████████████████             | 77/99 [00:36<00:10,  2.17it/s, loss=-0.05560]\u001b[A\n",
      "Epoch 1:  78%|█████████████████████████████████████████████             | 77/99 [00:36<00:10,  2.17it/s, loss=-0.05711]\u001b[A\n",
      "Epoch 1:  79%|█████████████████████████████████████████████▋            | 78/99 [00:36<00:09,  2.20it/s, loss=-0.05711]\u001b[A\n",
      "Epoch 1:  79%|█████████████████████████████████████████████▋            | 78/99 [00:36<00:09,  2.20it/s, loss=-0.05677]\u001b[A\n",
      "Epoch 1:  80%|██████████████████████████████████████████████▎           | 79/99 [00:37<00:09,  2.21it/s, loss=-0.05677]\u001b[A\n",
      "Epoch 1:  80%|██████████████████████████████████████████████▎           | 79/99 [00:37<00:09,  2.21it/s, loss=-0.05713]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████████▊           | 80/99 [00:37<00:08,  2.22it/s, loss=-0.05713]\u001b[A\n",
      "Epoch 1:  81%|██████████████████████████████████████████████▊           | 80/99 [00:37<00:08,  2.22it/s, loss=-0.05558]\u001b[A\n",
      "Epoch 1:  82%|███████████████████████████████████████████████▍          | 81/99 [00:38<00:08,  2.19it/s, loss=-0.05558]\u001b[A\n",
      "Epoch 1:  82%|███████████████████████████████████████████████▍          | 81/99 [00:38<00:08,  2.19it/s, loss=-0.05350]\u001b[A\n",
      "Epoch 1:  83%|████████████████████████████████████████████████          | 82/99 [00:38<00:07,  2.21it/s, loss=-0.05350]\u001b[A\n",
      "Epoch 1:  83%|████████████████████████████████████████████████          | 82/99 [00:38<00:07,  2.21it/s, loss=-0.05254]\u001b[A\n",
      "Epoch 1:  84%|████████████████████████████████████████████████▋         | 83/99 [00:39<00:07,  2.21it/s, loss=-0.05254]\u001b[A\n",
      "Epoch 1:  84%|████████████████████████████████████████████████▋         | 83/99 [00:39<00:07,  2.21it/s, loss=-0.05220]\u001b[A\n",
      "Epoch 1:  85%|█████████████████████████████████████████████████▏        | 84/99 [00:39<00:06,  2.18it/s, loss=-0.05220]\u001b[A\n",
      "Epoch 1:  85%|█████████████████████████████████████████████████▏        | 84/99 [00:39<00:06,  2.18it/s, loss=-0.05326]\u001b[A\n",
      "Epoch 1:  86%|█████████████████████████████████████████████████▊        | 85/99 [00:40<00:06,  2.19it/s, loss=-0.05326]\u001b[A\n",
      "Epoch 1:  86%|█████████████████████████████████████████████████▊        | 85/99 [00:40<00:06,  2.19it/s, loss=-0.05104]\u001b[A\n",
      "Epoch 1:  87%|██████████████████████████████████████████████████▍       | 86/99 [00:40<00:05,  2.19it/s, loss=-0.05104]\u001b[A\n",
      "Epoch 1:  87%|██████████████████████████████████████████████████▍       | 86/99 [00:40<00:05,  2.19it/s, loss=-0.05160]\u001b[A\n",
      "Epoch 1:  88%|██████████████████████████████████████████████████▉       | 87/99 [00:40<00:05,  2.20it/s, loss=-0.05160]\u001b[A\n",
      "Epoch 1:  88%|██████████████████████████████████████████████████▉       | 87/99 [00:40<00:05,  2.20it/s, loss=-0.05027]\u001b[A\n",
      "Epoch 1:  89%|███████████████████████████████████████████████████▌      | 88/99 [00:41<00:04,  2.21it/s, loss=-0.05027]\u001b[A\n",
      "Epoch 1:  89%|███████████████████████████████████████████████████▌      | 88/99 [00:41<00:04,  2.21it/s, loss=-0.05161]\u001b[A\n",
      "Epoch 1:  90%|████████████████████████████████████████████████████▏     | 89/99 [00:41<00:04,  2.22it/s, loss=-0.05161]\u001b[A\n",
      "Epoch 1:  90%|████████████████████████████████████████████████████▏     | 89/99 [00:41<00:04,  2.22it/s, loss=-0.05244]\u001b[A\n",
      "Epoch 1:  91%|████████████████████████████████████████████████████▋     | 90/99 [00:42<00:04,  2.21it/s, loss=-0.05244]\u001b[A\n",
      "Epoch 1:  91%|████████████████████████████████████████████████████▋     | 90/99 [00:42<00:04,  2.21it/s, loss=-0.05358]\u001b[A\n",
      "Epoch 1:  92%|█████████████████████████████████████████████████████▎    | 91/99 [00:42<00:03,  2.21it/s, loss=-0.05358]\u001b[A\n",
      "Epoch 1:  92%|█████████████████████████████████████████████████████▎    | 91/99 [00:42<00:03,  2.21it/s, loss=-0.05377]\u001b[A\n",
      "Epoch 1:  93%|█████████████████████████████████████████████████████▉    | 92/99 [00:43<00:03,  2.21it/s, loss=-0.05377]\u001b[A\n",
      "Epoch 1:  93%|█████████████████████████████████████████████████████▉    | 92/99 [00:43<00:03,  2.21it/s, loss=-0.05510]\u001b[A\n",
      "Epoch 1:  94%|██████████████████████████████████████████████████████▍   | 93/99 [00:43<00:02,  2.22it/s, loss=-0.05510]\u001b[A\n",
      "Epoch 1:  94%|██████████████████████████████████████████████████████▍   | 93/99 [00:43<00:02,  2.22it/s, loss=-0.05439]\u001b[A\n",
      "Epoch 1:  95%|███████████████████████████████████████████████████████   | 94/99 [00:44<00:02,  2.20it/s, loss=-0.05439]\u001b[A\n",
      "Epoch 1:  95%|███████████████████████████████████████████████████████   | 94/99 [00:44<00:02,  2.20it/s, loss=-0.05236]\u001b[A\n",
      "Epoch 1:  96%|███████████████████████████████████████████████████████▋  | 95/99 [00:44<00:01,  2.21it/s, loss=-0.05236]\u001b[A\n",
      "Epoch 1:  96%|███████████████████████████████████████████████████████▋  | 95/99 [00:44<00:01,  2.21it/s, loss=-0.05272]\u001b[A\n",
      "Epoch 1:  97%|████████████████████████████████████████████████████████▏ | 96/99 [00:45<00:01,  2.21it/s, loss=-0.05272]\u001b[A\n",
      "Epoch 1:  97%|████████████████████████████████████████████████████████▏ | 96/99 [00:45<00:01,  2.21it/s, loss=-0.05175]\u001b[A\n",
      "Epoch 1:  98%|████████████████████████████████████████████████████████▊ | 97/99 [00:45<00:00,  2.19it/s, loss=-0.05175]\u001b[A\n",
      "Epoch 1:  98%|████████████████████████████████████████████████████████▊ | 97/99 [00:45<00:00,  2.19it/s, loss=-0.05176]\u001b[A\n",
      "Epoch 1:  99%|█████████████████████████████████████████████████████████▍| 98/99 [00:45<00:00,  2.20it/s, loss=-0.05176]\u001b[A\n",
      "Epoch 1:  99%|█████████████████████████████████████████████████████████▍| 98/99 [00:45<00:00,  2.20it/s, loss=-0.05282]\u001b[A\n",
      "Epoch 1: 100%|██████████████████████████████████████████████████████████| 99/99 [00:46<00:00,  2.21it/s, loss=-0.05282]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████████████| 99/99 [00:46<00:00,  2.21it/s, loss=-0.05298]\u001b[A\n",
      "Epoch 1: 100%|█████████████████| 99/99 [01:22<00:00,  2.21it/s, loss=-0.05298, test_loss=-0.04928, train_loss=-0.06863]\u001b[A\n",
      "Epoch 1: 100%|█████████████████| 99/99 [01:22<00:00,  1.19it/s, loss=-0.05298, test_loss=-0.04928, train_loss=-0.06863]\u001b[A\n",
      "Epoch 2:   0%|                                                                                  | 0/99 [00:00<?, ?it/s]C:\\Users\\seidm\\miniconda3\\envs\\ddow\\lib\\site-packages\\threadpoolctl.py:1010: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "Epoch 2: 100%|█████████████████| 99/99 [01:21<00:00,  1.22it/s, loss=-0.07282, test_loss=-0.06217, train_loss=-0.08441]\n",
      "Epoch 3:   0%|                                                                                  | 0/99 [00:00<?, ?it/s]C:\\Users\\seidm\\miniconda3\\envs\\ddow\\lib\\site-packages\\threadpoolctl.py:1010: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "Epoch 3: 100%|█████████████████| 99/99 [01:20<00:00,  1.22it/s, loss=-0.09018, test_loss=-0.07882, train_loss=-0.10191]\n",
      "Epoch 4:   0%|                                                                                  | 0/99 [00:00<?, ?it/s]C:\\Users\\seidm\\miniconda3\\envs\\ddow\\lib\\site-packages\\threadpoolctl.py:1010: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "Epoch 4: 100%|█████████████████| 99/99 [01:20<00:00,  1.23it/s, loss=-0.11097, test_loss=-0.10755, train_loss=-0.12703]\n",
      "Epoch 5:   0%|                                                                                  | 0/99 [00:00<?, ?it/s]C:\\Users\\seidm\\miniconda3\\envs\\ddow\\lib\\site-packages\\threadpoolctl.py:1010: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "Epoch 5: 100%|█████████████████| 99/99 [01:26<00:00,  1.14it/s, loss=-0.12633, test_loss=-0.10940, train_loss=-0.13261]\n",
      "Epoch 6:   0%|                                                                                  | 0/99 [00:00<?, ?it/s]C:\\Users\\seidm\\miniconda3\\envs\\ddow\\lib\\site-packages\\threadpoolctl.py:1010: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "Epoch 6: 100%|█████████████████| 99/99 [01:30<00:00,  1.09it/s, loss=-0.13100, test_loss=-0.12366, train_loss=-0.13802]\n",
      "Epoch 7:   0%|                                                                                  | 0/99 [00:00<?, ?it/s]C:\\Users\\seidm\\miniconda3\\envs\\ddow\\lib\\site-packages\\threadpoolctl.py:1010: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "Epoch 7: 100%|█████████████████| 99/99 [01:35<00:00,  1.03it/s, loss=-0.13339, test_loss=-0.12225, train_loss=-0.13896]\n",
      "Epoch 8:   0%|                                                                                  | 0/99 [00:00<?, ?it/s]C:\\Users\\seidm\\miniconda3\\envs\\ddow\\lib\\site-packages\\threadpoolctl.py:1010: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "Epoch 8: 100%|█████████████████| 99/99 [01:33<00:00,  1.06it/s, loss=-0.13928, test_loss=-0.13606, train_loss=-0.14444]\n",
      "Epoch 9:   0%|                                                                                  | 0/99 [00:00<?, ?it/s]C:\\Users\\seidm\\miniconda3\\envs\\ddow\\lib\\site-packages\\threadpoolctl.py:1010: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "Epoch 9: 100%|█████████████████| 99/99 [01:27<00:00,  1.13it/s, loss=-0.14242, test_loss=-0.14390, train_loss=-0.15049]\n",
      "Epoch 10:   0%|                                                                                 | 0/99 [00:00<?, ?it/s]C:\\Users\\seidm\\miniconda3\\envs\\ddow\\lib\\site-packages\\threadpoolctl.py:1010: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "Epoch 10: 100%|████████████████| 99/99 [01:25<00:00,  1.16it/s, loss=-0.14954, test_loss=-0.16034, train_loss=-0.15709]\n",
      "Epoch 11:   0%|                                                                                 | 0/99 [00:00<?, ?it/s]C:\\Users\\seidm\\miniconda3\\envs\\ddow\\lib\\site-packages\\threadpoolctl.py:1010: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "Epoch 11: 100%|████████████████| 99/99 [01:29<00:00,  1.10it/s, loss=-0.15380, test_loss=-0.16401, train_loss=-0.16651]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12:   0%|                                                                                 | 0/99 [00:00<?, ?it/s]C:\\Users\\seidm\\miniconda3\\envs\\ddow\\lib\\site-packages\\threadpoolctl.py:1010: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "Epoch 12: 100%|████████████████| 99/99 [01:29<00:00,  1.10it/s, loss=-0.15795, test_loss=-0.18612, train_loss=-0.17727]\n",
      "Epoch 13:   0%|                                                                                 | 0/99 [00:00<?, ?it/s]C:\\Users\\seidm\\miniconda3\\envs\\ddow\\lib\\site-packages\\threadpoolctl.py:1010: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "Epoch 13: 100%|█████████████████████████████████████████████████████████| 99/99 [00:48<00:00,  2.05it/s, loss=-0.15690]"
     ]
    }
   ],
   "source": [
    "history = run.launch(200)\n",
    "torch.save(network.state_dict(), 'network.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain the model on validation set with limited amount of epochs, and limited learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_retrained = RnnNetFullOpti3(5,  0.2, 'diagonal', 1)\n",
    "network_retrained.load_state_dict(network.state_dict())\n",
    "\n",
    "##!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# state_dict = torch.load('min_var_1.pth')\n",
    "# network_retrained.load_state_dict(state_dict)\n",
    "# ####\n",
    "\n",
    "\n",
    "print(network_retrained)\n",
    "network_retrained = network_retrained.train()\n",
    "loss = SharpeRatio()\n",
    "run = Run(network_retrained,\n",
    "          loss,\n",
    "          dataloader_train_for_retrain,\n",
    "          val_dataloaders={\n",
    "              'train': dataloader_train_for_retrain,\n",
    "              'test': dataloader_test_for_retrain\n",
    "              },\n",
    "          optimizer=torch.optim.Adam(network_retrained.parameters(), amsgrad=True, lr=0.0005),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_retrain = run.launch(50)\n",
    "torch.save(network_retrained.state_dict(), 'network_retrained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1ZAJiA3WIb8"
   },
   "source": [
    "# Check model perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_C6qYeh57N2",
    "outputId": "d7e4d6ec-0ea7-4cd8-85a1-db313452e03c"
   },
   "outputs": [],
   "source": [
    "  # network = LstmNetMinVar(5)\n",
    "  # network.load_state_dict(torch.load(fr'network_2.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5V7kXiXFfA29"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "per_epoch_results = history.metrics.groupby(['dataloader', 'metric', 'model', 'epoch'])['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "A5Dtu_QyeIrf",
    "outputId": "04d75fd0-5a81-4d6f-9546-967462c18605",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_test_loss = per_epoch_results.mean()['test']['loss']['network'] # tmp\n",
    "\n",
    "# Plotting the mean test loss per epoch\n",
    "mean_test_loss.plot()\n",
    "\n",
    "# Finding the epoch with the minimum test loss\n",
    "min_loss_epoch = mean_test_loss.idxmin()\n",
    "min_loss_value = mean_test_loss.min()\n",
    "\n",
    "# Adding a red vertical line at the epoch with minimum test loss\n",
    "plt.axvline(x=min_loss_epoch, color='r', linestyle='--', label=f'Min Loss at Epoch {min_loss_epoch}')\n",
    "\n",
    "# Adding some labels and title for clarity\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.title('Validation Loss per Epoch')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNjFRuqyp9R3"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E9tBNUrqa4qO",
    "outputId": "a4f5a9e3-da2c-4207-eee3-202084d6e215"
   },
   "outputs": [],
   "source": [
    "per_epoch_results.mean()['test']['loss']['network'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "on_gcl3zWx5p",
    "outputId": "18189bc7-f1ca-4fbc-ce19-cd5087369c35"
   },
   "outputs": [],
   "source": [
    "mean_train_loss = per_epoch_results.mean()['train']['loss']['network']\n",
    "\n",
    "# Plotting the mean test loss per epoch\n",
    "mean_train_loss.plot()\n",
    "\n",
    "\n",
    "\n",
    "# Adding some labels and title for clarity\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.title('Train Loss per Epoch')\n",
    "plt.axvline(x=min_loss_epoch, color='r', linestyle='--', label=f'The best model {min_loss_epoch}')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.tmp['output'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5eJGUoOGQbx"
   },
   "source": [
    "# Compute validation factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYwpyZGaDw08"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "FY1f1XULzwxF"
   },
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X[indices_test], dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y[indices_test], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seidm\\miniconda3\\envs\\ddow\\lib\\site-packages\\threadpoolctl.py:1010: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "weights = network(X_tensor)\n",
    "ewp = weights.new_full(weights.shape, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0457, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(weights, y_tensor).mean()  /  loss(ewp, y_tensor).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8629, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPY_long = torch.zeros(742, 5)\n",
    "SPY_long[:, 1] = 1\n",
    "loss(weights, y_tensor).mean()  /  loss(SPY_long, y_tensor).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(SPY_long, y_tensor).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "BlApRJuPfrxq",
    "VNqxm-tdcv3Y",
    "P5eJGUoOGQbx"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
